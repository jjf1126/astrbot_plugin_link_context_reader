import re
import asyncio
import traceback
import base64
import json
from typing import Optional, List, Dict, Tuple
from urllib.parse import urlparse, quote, parse_qs

import aiohttp
from bs4 import BeautifulSoup

# å°è¯•å¯¼å…¥ Playwright æˆªå›¾ç»„ä»¶
try:
    from playwright.async_api import async_playwright
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False

from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger
from astrbot.api.provider import ProviderRequest

@register("astrbot_plugin_link_reader", "AstrBot_Developer", "è‡ªåŠ¨è§£æé“¾æ¥å†…å®¹ï¼Œæ”¯æŒå…¨å¹³å°éŸ³ä¹çŸ­é“¾è¿½è¸ªåŠæ·±åº¦æ­Œè¯è§£æã€‚", "1.6.0")
class LinkReaderPlugin(Star):
    def __init__(self, context: Context, config: dict):
        super().__init__(context)
        self.config = config
        
        # åŠ è½½åŸºç¡€é…ç½®
        self.general_config = self.config.get("general_config", {})
        self.enable_plugin = self.general_config.get("enable_plugin", True)
        self.max_length = self.general_config.get("max_content_length", 2000)
        self.timeout = self.general_config.get("request_timeout", 15)
        self.user_agent = self.general_config.get("user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        self.prompt_template = self.general_config.get("prompt_template", "\nã€ä»¥ä¸‹æ˜¯é“¾æ¥çš„å…·ä½“å†…å®¹ï¼Œè¯·å‚è€ƒè¯¥å†…å®¹è¿›è¡Œå›ç­”ã€‘ï¼š\n{content}\n")

        # åŠ è½½å¹³å° Cookie
        self.platform_cookies = self.config.get("platform_cookies", {})

        # URL åŒ¹é…æ­£åˆ™
        self.url_pattern = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w\.-]*\??[\w=&%\.-]*')

    def _get_headers(self, domain: str = "") -> dict:
        """æ ¹æ®åŸŸåè·å–å¯¹åº”çš„ Headers (åŒ…å« Cookie)"""
        headers = {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
        cookie_key = None
        if "xiaohongshu" in domain: cookie_key = "xiaohongshu"
        elif "zhihu" in domain: cookie_key = "zhihu"
        elif "weibo" in domain: cookie_key = "weibo"
        elif "bilibili" in domain: cookie_key = "bilibili"
        elif "douyin" in domain: cookie_key = "douyin"
        elif "tieba.baidu" in domain: cookie_key = "tieba"
        elif "lofter" in domain: cookie_key = "lofter"

        if cookie_key:
            cookie_val = self.platform_cookies.get(cookie_key, "")
            if cookie_val:
                headers["Cookie"] = cookie_val
        return headers

    def _is_music_site(self, url: str) -> bool:
        """å…¨å¹³å°éŸ³ä¹åŸŸåè¯†åˆ«ï¼ˆå«å„ç±»çŸ­é“¾åŸŸåï¼‰"""
        music_domains = [
            # ç½‘æ˜“äº‘
            "music.163.com", "163cn.tv", "163.fm", "y.music.163.com",
            # QQ éŸ³ä¹
            "y.qq.com", "c6.y.qq.com", "c.y.qq.com", "u.y.qq.com", "url.cn",
            # é…·ç‹—
            "kugou.com", "t.kugou.com", "fanxing.kugou.com",
            # é…·æˆ‘
            "kuwo.cn", "t.kuwo.cn",
            # å’ªå’•/Bç«™éŸ³ä¹ç­‰
            "migu.cn", "b23.tv"
        ]
        return any(domain in url for domain in music_domains)

    def _contains_chinese(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«æ±‰å­—"""
        for char in text:
            if '\u4e00' <= char <= '\u9fff':
                return True
        return False

    def _filter_lyrics(self, lyrics: str) -> str:
        """æ·±åº¦æ¸…æ´—é€»è¾‘ï¼Œå»é™¤å…ƒæ•°æ®å’Œæ—¶é—´è½´"""
        if not lyrics: return ""
        lyrics = lyrics.replace('\\n', '\n').replace('\\r', '')
        lines = lyrics.split('\n')
        filtered_lines = []
        for line in lines:
            line = line.strip()
            if not line: continue
            line = re.sub(r'\[\d+:\d+\.\d+\]', '', line).strip()
            if not line or (line.startswith('[') and line.endswith(']')): continue
            
            if ((':' in line or 'ï¼š' in line) and len(line) < 35) or ' - ' in line:
                if not any(kw in line for kw in ["æ­Œè¯", "Lyric", "LRC", "æ–‡æœ¬"]):
                    continue
            
            if ' ' in line and self._contains_chinese(line):
                parts = [part.strip() for part in line.split(' ') if part.strip()]
                if all(len(part) < 20 for part in parts):
                    filtered_lines.extend(parts)
                    continue
            
            filtered_lines.append(line)
        
        final_lines = [l for l in filtered_lines if len(l) > 1 and not l.isdigit()]
        return '\n'.join(final_lines)

    def _clean_text(self, text: str) -> str:
        """å¸¸è§„ç½‘é¡µæ¸…æ´—é€»è¾‘"""
        lines = text.split('\n')
        blacklist = ["æ²ªICPå¤‡", "å…¬ç½‘å®‰å¤‡", "ç»è¥è®¸å¯è¯", "ç‰ˆæƒæ‰€æœ‰", "Â©", "Copyright", "ä¸‹è½½APP", "æ‰“å¼€APP"]
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if not line or len(line) < 2 or any(kw in line for kw in blacklist):
                continue
            cleaned_lines.append(line)
        result = '\n'.join(cleaned_lines)
        if len(result) > self.max_length:
            result = result[:self.max_length] + "...(å†…å®¹è¿‡é•¿å·²æˆªæ–­)"
        return result

    async def _handle_music_direct_api(self, url: str) -> str:
        """éŸ³ä¹ç›´è¿è§£æå…¥å£ï¼šæ”¯æŒå…¨å¹³å°çŸ­é“¾è¿½è¸ª"""
        try:
            async with aiohttp.ClientSession() as session:
                # 1. å…¨å¹³å°çŸ­é“¾æ¥è¿½è¸ª (Redirection Tracking)
                final_url = url
                short_link_domains = [
                    "163cn.tv", "163.fm", "url.cn", "c6.y.qq.com", 
                    "u.y.qq.com", "t.kugou.com", "t.kuwo.cn", "b23.tv"
                ]
                if any(domain in url for domain in short_link_domains) or "base/fcgi-bin/u" in url:
                    headers = {"User-Agent": self.user_agent}
                    async with session.head(url, allow_redirects=True, timeout=8, headers=headers) as resp:
                        final_url = str(resp.url)
                        logger.info(f"[LinkReader] çŸ­é“¾é‡å®šå‘æˆåŠŸ: {url} -> {final_url}")

                # --- å¹³å°è§£æ: ç½‘æ˜“äº‘ ---
                if "music.163.com" in final_url:
                    id_match = re.search(r'id=(\d+)', final_url) or re.search(r'song/(\d+)', final_url)
                    if id_match:
                        song_id = id_match.group(1)
                        api_url = f"https://music.163.com/api/song/lyric?id={song_id}&lv=-1&tv=-1"
                        headers = {"Referer": "https://music.163.com/", "Cookie": "os=pc", "User-Agent": self.user_agent}
                        async with session.get(api_url, headers=headers) as resp:
                            text = await resp.text()
                            data = json.loads(text)
                            lrc = data.get("lrc", {}).get("lyric", "")
                            tlrc = data.get("tlyric", {}).get("lyric", "")
                            if lrc:
                                res = f"ã€ç½‘æ˜“äº‘è§£æ (ID: {song_id})ã€‘\n\n{self._filter_lyrics(lrc)}"
                                if tlrc: res += f"\n\nã€ç¿»è¯‘ã€‘\n{self._filter_lyrics(tlrc)}"
                                return res

                # --- å¹³å°è§£æ: QQ éŸ³ä¹ ---
                elif "y.qq.com" in final_url:
                    mid_match = re.search(r'songmid=([a-zA-Z0-9]+)', final_url) or re.search(r'songDetail/([a-zA-Z0-9]+)', final_url)
                    if mid_match:
                        mid = mid_match.group(1)
                        api_url = f"https://c.y.qq.com/lyric/fcgi-bin/fcg_query_lyric_new.fcg?songmid={mid}&format=json&nobase64=1"
                        headers = {"Referer": "https://y.qq.com/", "User-Agent": self.user_agent}
                        async with session.get(api_url, headers=headers) as resp:
                            text = await resp.text()
                            try:
                                data = json.loads(re.sub(r'^\w+\(|\)$', '', text))
                                lrc = data.get("lyric", "")
                                if lrc: return f"ã€QQéŸ³ä¹è§£æ (MID: {mid})ã€‘\n\n{self._filter_lyrics(lrc)}"
                            except: pass

                # --- å¹³å°è§£æ: é…·æˆ‘éŸ³ä¹ ---
                elif "kuwo.cn" in final_url:
                    id_match = re.search(r'mid=(\d+)', final_url) or re.search(r'musicId=(\d+)', final_url)
                    if id_match:
                        mid = id_match.group(1)
                        api_url = f"http://m.kuwo.cn/newh5/singles/songinfoandlrc?musicId={mid}"
                        async with session.get(api_url) as resp:
                            text = await resp.text()
                            data = json.loads(text)
                            lrc_list = data.get("data", {}).get("lrclist", [])
                            if lrc_list:
                                lrc_text = "\n".join([i['lineLyric'] for i in lrc_list])
                                return f"ã€é…·æˆ‘éŸ³ä¹è§£æã€‘\n\n{lrc_text}"

                # æ— æ³•ç›´è¿åˆ™è¿›å…¥å…œåº•æœç´¢
                return await self._fallback_xiaojiang_search(final_url)

        except Exception as e:
            logger.error(f"[LinkReader] éŸ³ä¹ API ç›´è¿å¼‚å¸¸: {e}")
            return await self._fallback_xiaojiang_search(url)

    async def _fallback_xiaojiang_search(self, url: str) -> str:
        """å…³é”®è¯ç²¾ç®€å…œåº•æœç´¢"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers={"User-Agent": self.user_agent}, timeout=8) as resp:
                    soup = BeautifulSoup(await resp.text(errors='ignore'), 'lxml')
                    title = soup.title.string.strip() if soup.title else "æœªçŸ¥æ­Œæ›²"
            
            # 1. åŸºç¡€æ¸…æ´—ï¼šç§»é™¤å¹³å°åç¼€
            song_name = re.sub(r'( - ç½‘æ˜“äº‘éŸ³ä¹| - QQéŸ³ä¹| - é…·ç‹—éŸ³ä¹| - é…·æˆ‘éŸ³ä¹|\|.*| - æ­Œæ›².*| - å•æ›²| - ä¸“è¾‘| - å’ªå’•éŸ³ä¹)$', '', title).strip()
            song_name = re.sub(r'^(æ­Œæ›²|å•æ›²|åˆ†äº«|æ­£åœ¨æ’­æ”¾)ï¼š', '', song_name)
            
            # 2. æ·±åº¦æ¸…æ´—ï¼šç§»é™¤æ‹¬å·ã€å‘¨å¹´æ›²ç­‰è£…é¥°æ€§å†…å®¹
            clean_name = re.sub(r'[ï¼ˆã€Š\(ã€].*?[ï¼‰ã€‹\)ã€‘]', '', song_name).strip()
            
            # 3. æ­Œæ‰‹æ‹†åˆ†ï¼šå–ç¬¬ä¸€ä¸ª '-' å‰åçš„æ ¸å¿ƒéƒ¨åˆ†
            if ' - ' in clean_name:
                parts = clean_name.split(' - ')
                # ä¼˜å…ˆä¿ç•™æ­Œåï¼Œè‹¥ç¬¬ä¸€éƒ¨åˆ†å¤ªçŸ­ï¼ˆå¦‚ç¬¦å·ï¼‰åˆ™å–ç¬¬äºŒéƒ¨åˆ†
                clean_name = parts[0].strip() if len(parts[0].strip()) > 1 else parts[1].strip()
            
            # 4. å®‰å…¨æ ¡éªŒï¼šé˜²æ­¢æœå‡ºç©ºå­—ç¬¦ä¸²æˆ–çº¯ç¬¦å·
            final_keyword = clean_name if len(re.sub(r'[^\w\u4e00-\u9fff]', '', clean_name)) >= 1 else song_name
            if not final_keyword: final_keyword = title[:15]

            logger.info(f"[LinkReader] è§¦å‘å…œåº•æœç´¢ï¼Œå…³é”®è¯: {final_keyword}")
            content = await self._search_xiaojiang(final_keyword)
            
            if content:
                return f"ã€æ­Œè¯è§£æ: {final_keyword}ã€‘\næ¥æº: å°æ±ŸéŸ³ä¹ç½‘\n\n{content}"
            return f"è¯†åˆ«åˆ°éŸ³ä¹é“¾æ¥ï¼Œä½†åœ¨æœç´¢ã€Š{final_keyword}ã€‹æ—¶æœªèƒ½åŒ¹é…åˆ°æ­Œè¯æ­£æ–‡ã€‚"
        except Exception:
            return "éŸ³ä¹é“¾æ¥è§£æå¤±è´¥ã€‚"

    async def _search_xiaojiang(self, song_name: str) -> Optional[str]:
        """å°æ±ŸéŸ³ä¹ç½‘æœç´¢é€»è¾‘"""
        search_url = f"https://xiaojiangclub.com/?s={quote(song_name)}"
        base_domain = "https://xiaojiangclub.com"
        headers = {"User-Agent": self.user_agent}
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(search_url, headers=headers, timeout=10) as resp:
                    if resp.status != 200: return None
                    soup = BeautifulSoup(await resp.text(), 'lxml')
                    
                    target_link_tag = soup.find('a', class_='song-link', href=True)
                    if not target_link_tag: return None
                    
                    target_path = target_link_tag['href']
                    target_link = target_path if target_path.startswith("http") else base_domain + target_path
                    
                    async with session.get(target_link, headers=headers, timeout=10) as l_resp:
                        l_soup = BeautifulSoup(await l_resp.text(), 'lxml')
                        container = l_soup.find('div', class_='entry-content') or l_soup.find('article') or l_soup.find('div', class_='post-content')
                        if not container: container = l_soup
                        
                        for tag in container(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'button']):
                            tag.decompose()
                            
                        return self._filter_lyrics(container.get_text(separator='\n', strip=True))
        except: pass
        return None

    async def _get_screenshot_and_content(self, url: str):
        """Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–æˆªå›¾"""
        if not HAS_PLAYWRIGHT: return None, None
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(user_agent=self.user_agent, viewport={'width': 1280, 'height': 800})
                page = await context.new_page()
                await page.goto(url, wait_until='networkidle', timeout=30000)
                content = await page.content()
                screenshot_bytes = await page.screenshot(type='jpeg', quality=80)
                screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
                await browser.close()
                return content, screenshot_base64
        except Exception as e:
            logger.error(f"[LinkReader] æˆªå›¾å¤±è´¥: {e}")
            return None, None

    async def _fetch_url_content(self, url: str):
        """ç½‘é¡µæŠ“å–ä¸»å…¥å£"""
        if self._is_music_site(url):
            return await self._handle_music_direct_api(url), None
        
        domain = urlparse(url).netloc
        social_platforms = ["xiaohongshu.com", "zhihu.com", "weibo.com", "bilibili.com", "douyin.com", "lofter.com"]
        if any(sp in domain for sp in social_platforms) and HAS_PLAYWRIGHT:
            html, screenshot = await self._get_screenshot_and_content(url)
            if html:
                soup = BeautifulSoup(html, 'lxml')
                for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe']): tag.decompose()
                content = soup.get_text(separator='\n', strip=True)
                return self._clean_text(content), screenshot

        headers = self._get_headers(domain)
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10, ssl=False) as resp:
                    soup = BeautifulSoup(await resp.text(errors='ignore'), 'lxml')
                    for tag in soup(['script', 'style', 'nav', 'footer', 'header']): tag.decompose()
                    return self._clean_text(soup.get_text(separator='\n', strip=True)), None
        except Exception as e:
            return f"ç½‘é¡µè§£æå‡ºé”™: {str(e)}", None

    @filter.on_llm_request()
    async def on_llm_request(self, event: AstrMessageEvent, req: ProviderRequest):
        """æ‹¦æˆª LLM è¯·æ±‚æ³¨å…¥ä¸Šä¸‹æ–‡"""
        if not self.enable_plugin: return
        urls = self.url_pattern.findall(event.message_str)
        if not urls: return
        
        target_url = urls[0]
        content, screenshot_base64 = await self._fetch_url_content(target_url)

        if content:
            req.prompt += self.prompt_template.format(content=content)
            if screenshot_base64:
                req.prompt += f"\n(é™„å¸¦é¡µé¢æˆªå›¾)\nå›¾ç‰‡ï¼šdata:image/jpeg;base64,{screenshot_base64}"
            logger.info(f"[LinkReader] æˆåŠŸæ³¨å…¥é“¾æ¥å†…å®¹")

    @filter.command("link_debug")
    async def link_debug(self, event: AstrMessageEvent, url: str):
        """è°ƒè¯•æŒ‡ä»¤"""
        if not url: return
        yield event.plain_result(f"ğŸ” æ­£åœ¨æ·±åº¦è§£æé“¾æ¥: {url}...")
        content, screenshot = await self._fetch_url_content(url)
        msg = f"ã€è§£ææ­£æ–‡å†…å®¹ã€‘:\n{content}"
        yield event.plain_result(msg)

    @filter.command("link_status")
    async def link_status(self, event: AstrMessageEvent):
        """çŠ¶æ€æ£€æŸ¥"""
        msg = [
            "ã€Link Reader 1.6.0 çŠ¶æ€æŠ¥å‘Šã€‘",
            "å…¨å¹³å°çŸ­é“¾è¿½è¸ª: âœ… (163/QQ/Kugou/Kuwo/Bili)",
            "éŸ³ä¹ç›´è¿ API: âœ… (ç½‘æ˜“/QQ/é…·æˆ‘)",
            "æ™ºèƒ½ç²¾ç®€æœç´¢: âœ… (XiaojiangClub)",
            f"æ­£æ–‡æœ€å¤§é•¿åº¦: {self.max_length}"
        ]
        yield event.plain_result("\n".join(msg))
